{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, you need to create a machine learning model that will classify SMS messages as either \"ham\" or \"spam\". A \"ham\" message is a normal message sent by a friend. A \"spam\" message is an advertisement or a message sent by a company.\n",
        "\n",
        "You should create a function called predict_message that takes a message string as an argument and returns a list. The first element in the list should be a number between zero and one that indicates the likeliness of \"ham\" (0) or \"spam\" (1). The second element in the list should be the word \"ham\" or \"spam\", depending on which is most likely.\n",
        "\n",
        "For this challenge, you will use the SMS Spam Collection dataset. The dataset has already been grouped into train data and test data."
      ],
      "metadata": {
        "id": "pGJ-U2MDF9qB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "names = [\"class\", \"message\"]\n",
        "train_file = pd.read_csv(train_file_path, sep='\\t', names=names)\n",
        "test_file = pd.read_csv(test_file_path, sep='\\t', names=names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "train_file.loc[train_file['class'] == 'ham', 'class'] = 0\n",
        "train_file.loc[train_file['class'] == 'spam', 'class'] = 1\n",
        "\n",
        "test_file.loc[test_file['class'] == 'ham', 'class'] = 0\n",
        "test_file.loc[test_file['class'] == 'spam', 'class'] = 1\n",
        "\n",
        "train_message = train_file['message'].values.tolist()\n",
        "train_label = train_file['class'].values\n",
        "test_message = test_file['message'].values.tolist()\n",
        "test_label = test_file['class'].values\n",
        "\n",
        "train_label = train_label.astype('int32')\n",
        "test_label = test_label.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Step 1: Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")  # You can adjust num_words\n",
        "\n",
        "# Step 2: Fit the tokenizer on the training data\n",
        "tokenizer.fit_on_texts(train_message)"
      ],
      "metadata": {
        "id": "C33n2qfBL3LL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_message)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_message)"
      ],
      "metadata": {
        "id": "x1SkDJVGMLLb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences[0]  #contains the tokenized integer sequence representation of the first message from train_message[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Fc_jGc70MVaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenizer.word_index)\n",
        "\n",
        "# inv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
        "# decoded = [inv_vocab.get(i, \"<OOV>\") for i in train_sequences[0]]\n",
        "# print(decoded)"
      ],
      "metadata": {
        "id": "mbcNGh65Mq98"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate VOCAB_SIZE including the OOV token\n",
        "# The tokenizer's word_index does not include the OOV token by default.\n",
        "# Since index 0 is reserved for the OOV token, the total vocabulary size\n",
        "# for the embedding layer is the number of unique words + 1 for the OOV token.\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "max_len = max(len(seq) for seq in train_sequences)"
      ],
      "metadata": {
        "id": "7waWbwBANHU9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Pad the sequences (to ensure all have same length)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "-_T_r5q0Pts6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 100),\n",
        "    # tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "model.fit(train_padded, train_label, epochs=30, validation_data=(test_padded, test_label), callbacks=[early_stop])\n"
      ],
      "metadata": {
        "id": "y2Gr3tnOZcn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c88bdf-8632-4c90-90aa-cd22cf75c3c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - acc: 0.8664 - loss: 0.3188 - val_acc: 0.9763 - val_loss: 0.0966\n",
            "Epoch 2/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - acc: 0.9847 - loss: 0.0592 - val_acc: 0.9820 - val_loss: 0.0516\n",
            "Epoch 3/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - acc: 0.9930 - loss: 0.0252 - val_acc: 0.9864 - val_loss: 0.0406\n",
            "Epoch 4/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - acc: 0.9975 - loss: 0.0121 - val_acc: 0.9871 - val_loss: 0.0402\n",
            "Epoch 5/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - acc: 0.9993 - loss: 0.0068 - val_acc: 0.9864 - val_loss: 0.0401\n",
            "Epoch 6/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - acc: 0.9997 - loss: 0.0044 - val_acc: 0.9892 - val_loss: 0.0347\n",
            "Epoch 7/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - acc: 0.9998 - loss: 0.0033 - val_acc: 0.9871 - val_loss: 0.0385\n",
            "Epoch 8/30\n",
            "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - acc: 0.9996 - loss: 0.0032 - val_acc: 0.9871 - val_loss: 0.0386\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bb4d3955590>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Why the Simple Model Performs Better\n",
        "\n",
        "Your simple model:\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 100),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "- Uses an Embedding layer to convert words into vectors.\n",
        "\n",
        "- Applies Flatten, which ignores word order and treats the entire sequence as a bag of embeddings.\n",
        "\n",
        "- This is often enough for tasks like spam detection, where certain keywords (\"free\", \"win\", \"offer\") strongly indicate spam.\n",
        "\n",
        "- Has fewer parameters, making it less prone to overfitting and fast to train.\n",
        "\n",
        "- Performs well on small and simple datasets.\n",
        "\n",
        "ğŸ“ˆ As a result, this model can easily reach very high accuracy (even close to 100%) when the classification task is largely based on presence of specific words."
      ],
      "metadata": {
        "id": "ArBY8tJHnRrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âŒ Why the LSTM Model Underperforms\n",
        "Your LSTM model:\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 100),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "- Introduces a lot more parameters, making it more complex and harder to train, especially on small datasets.\n",
        "\n",
        "- Tries to learn word order and contextual relationships, which may not be crucial for spam classification.\n",
        "\n",
        "- Requires more data and longer training to show its strengths.\n",
        "\n",
        "- Is more prone to underfitting or overfitting without proper regularization.\n",
        "\n",
        "âš ï¸ On small datasets like SMS spam, the LSTM might not have enough signal to learn useful patterns and may struggle to match the performance of simpler models."
      ],
      "metadata": {
        "id": "IN00WlQloO--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "J9tD9yACG6M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79798cf4-a658-4c18-f604-8a32cf06638b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "[np.float32(0.58725935), 'spam']\n"
          ]
        }
      ],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "\n",
        "    sequence = tokenizer.texts_to_sequences([pred_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    prob = model.predict(padded)[0][0]  # Get the probability from the prediction output\n",
        "\n",
        "    label = 'spam' if prob > 0.5 else 'ham'\n",
        "    return [prob, label]\n",
        "\n",
        "pred_text = \"sale today! to stop texts call 98912460324\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Dxotov85SjsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb261c5-c012-4b3b-a9a1-653061255389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "[np.float32(0.0012677497), 'ham']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "[np.float32(0.58725935), 'spam']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "[np.float32(7.012616e-05), 'ham']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "[np.float32(0.99209875), 'spam']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "[np.float32(0.99884593), 'spam']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "[np.float32(0.00042806624), 'ham']\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "[np.float32(0.0010556177), 'ham']\n",
            "You passed the challenge. Great job!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won Â£1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    print(prediction)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {},
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}